\documentclass[uplatex, twocolumn,10pt]{jsarticle}

\usepackage[dvipdfmx]{graphicx}
\usepackage{latexsym}
\usepackage{bmpsize}
\usepackage{url}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{ltablex}
\usepackage{enumitem}

\usepackage{booktabs}

\def\Underline{\setbox0\hbox\bgroup\let\\\endUnderline}
\def\endUnderline{\vphantom{y}\egroup\smash{\underline{\box0}}\\}

\newcommand{\ttt}[1]{\texttt{#1}}

\begin{document}

\title{
    \bf{
        \LARGE{An OpenCV-based Framework for Table Information Extraction} \\
        \Large{OpenCVを用いた表情報抽出のためのフレームワーク}
    }
}
\author{ {J. Yuan, H. Li, M. Wang, R. Liu, C. Li \and B. Wang,} , \\
    2020 IEEE International Conference on Knowledge Graph (ICKG), \\
    2020, pp.621-628 \\
    doi: 10.1109/ICBK50248.2020.00093.\\ }
\date{訳: 木村 優哉 \\ 2025年5月30日(金)}

\maketitle


\begin{abstract}
    PDF (Portable Document Format) は最も普及したファイル形式の 1 つであり、特に教科書や論文などの教育文書において、元のグラフィック外観を保持しオンラインで容易に共有できるため有用である。
    PDF ファイル内の表から情報を検出・抽出することで、教育的な知識グラフを構築するための豊富な構造化データを提供できる。
    しかし、既存の手法の多数は PDF パースツールや自然言語処理技術に依存しており、一般的に学習サンプルを必要とし、ページをまたがる表の処理が困難である。
    そこで本論文では、PDF の表からメタデータと具体的な値を抽出するための新しい OpenCV ベースのフレームワークを提案する。
    具体的には、まず表の視覚的な輪郭を強調し、水平・垂直の線を用いて表を特定し、各 PDF ページにおける表フレームの座標を取得する。
    表が検出されると、各表に対してページをまたがるケースを検出し、光学式文字認識 (OCR) エンジンを使用して各表セル内の具体的な値を抽出する。
    機械学習ベースの他の手法と異なり、提案手法はラベル付きデータなしで正確に表情報を抽出できる。
    実世界の PDF ファイルで広範な実験を行い、結果は本アプローチがページをまたがる表に効果的に対応でき、1 つの表の処理に平均 6.12 秒しかかからないことを示している。\\
    \textbf{キーワード}: 情報抽出、PDF、OpenCV
\end{abstract}



\section{はじめに}

表は、情報表現の普遍的な形式として、科学論文、Web ページ、PDF 形式の財務報告書など、さまざまなプラットフォームで広く利用されている。
表は、研究者が表現したい実験データや統計データを、非常に簡潔かつ明確な二次元の構造化形式で示すものである。
多くの研究者によって、重要なメタデータや値を含む情報を取得するための表の検出と抽出が行われており、これは知識グラフの構築を支援することもある。
しかし、人間が PDF 文書から表情報を認識することは難しくないものの、これらのデータを自動的に抽出することは困難である。
このタスクを抽出する上での主な課題は、表はその凝縮された形式のために通常ファイルの小さな部分しか占有せず、他の要素と混在するため、事前に表を検出するための適切なアプローチを見つけることが難しいという点である。
さらに、PDF 文書内の表はほとんどの場合タグ付けされていない。
上記の 2 点は、研究者が適切な表抽出方法を提案することを非常に困難にしている。

Yildiz ら[1]や Ramel ら[2]などの既存の手法は、ある程度、表抽出技術の設計に成功しているが、依然として制約が存在する。
これらの制約には、PDF から HTML への変換ツールの精度への依存や、PDF 文書に関する事前定義された仮定が含まれる。
また、色付きの表、並列した表、および、複数ページにまたがる表の抽出について言及した論文はほとんどないことがわかった。
これらの制限を克服するために、我々は電子文書における表の検出と抽出を自動的に行うだけでなく、特殊な表要素や複数ページにまたがる表の処理ステップを追加した手法を提案する。

我々の手法は、PDF 文書における表理解のために、概ね 3 つの連続したステップで構成される。
第一に、PDF ファイル内の各ページに対して前処理ステップを実行し、表内の特殊な要素を処理可能な要素に変換する。
第二に、水平線と垂直線を用いて表を特定し、各 PDF ページにおける表フレームの座標を取得する。
表が正常に検出されると、各表について、表内のすべてのセルを取得し、光学式文字認識 (OCR) エンジンを使用して各表セル内の文字を識別する。
一連の操作の後、PDF内の表からメタデータ (例えば、ヘッダー、フォント、データ型、座標) とデータを正常に取得する。
PDF内の表をページ順に抽出するため、次のページに切り替える際には、ページをまたがる表があるかどうかを判定するアルゴリズムを実行する。

本論文の貢献は以下の通りである：
\begin{itemize}
    \item PDF ファイルから表のメタデータとデータを自動的に抽出できる OpenCV ベースのフレームワークを紹介する。
    \item 表検出およびメタデータ抽出中に色付きの表を処理できるモデルを提案する。
    \item 並列した表や複数ページにまたがる表など、さまざまな種類の表を識別するアルゴリズムを設計する。
\end{itemize}

本論文の構成は以下の通りである。
2 章では、表処理に関する従来の研究と OpenCV ライブラリのいくつかの使用法を簡単に紹介する。
3 章では、我々の表抽出方法について詳細に説明する。
4 章では、我々の手法の精度を検証し、失敗例と限界を分析する。
5 章で我々の研究を結論を述べる。


\section{関連研究}

本章では、表処理技術およびシステムに関する関連研究の従来の研究を、表情報抽出に関する先駆的研究、OpenCVの使用法、PDF情報抽出の方法という観点から議論する。

\subsection{表情報抽出に関する先駆的研究}
20年前から多くの調査研究が現れているが、その多くは問題の特定の側面にのみ焦点を当ててきた。
LoprestiとNagy [3] は、電子的形式の表の分析が、より高度な表理解に不可欠な貢献者となり得ることを見出した。
彼らはまた、「表形式性」の定義や表形式ブラウジングについても検討した。
Zanibbiら [4] は、表モデル、観測、変換、推論を含む表認識プロセスを研究した。
彼らは、上記の要素の相互作用として表認識の文献を提示した。
Embleyら [5] は、Wang [6] によって提案されたモデルを分析し、それが明示的な情報が少ない有用なタスクに対して効果的であることを証明し、半自動表処理システムの開発計画の概要を述べた。

Hurst [7] は博士論文において、表理解のための抽象モデルを設計した。このモデルは、図的、物理的、機能的、構造的、意味的という5つの構成要素を含む。
彼 [8] は、表処理タスクを、表の位置特定、表認識、機能的および構造的分析、解釈という4つの手順に分割し、表の解釈をより容易にした。
彼のモデルは競争力のあるものであったが、彼は主にモデル自体に関心があり、表抽出と理解の詳細についてはあまり関心がなかった。
加えて、彼のプロセスの議論には、我々の手法の不可欠な部分である文書内の表を特定する能力が欠けていた。

特定の文書形式の問題を解決するために、さまざまな表抽出技術が使用されてきた。
表情報は、HTML、XML など、さまざまな文書形式に含まれている。
Chen ら[9]は、大規模なHTMLテキストから表を抽出する研究を発表した。
彼らは、文字列、名前付きエンティティ、および数値カテゴリの類似性を使用して、それが実際の表であるかどうかを判断した。
次に、これらのメトリックを適用してセルの類似性を計算し、表の読み取り方法 (行方向または列方向) を判断した。
Tengli ら[10]は、Web ページ内の HTML ページに焦点を当てた。
彼らは、<table>タグを他のタグ (<tr>、<td>、<th>、<caption>) とともに使用して、表の構造を取得し、その内容を解析した。

しかし、PDF ファイルにはバックエンドの HTML や XML がないため、彼らの手法を PDF ファイルに直接適用することはできない。
問題とアプリケーションの要件はさまざまであるため、表抽出に関する研究も多様な手法を用いている。
我々が扱う文書の特殊性に関して、我々のアプローチは、包括的なプロセスにより、タグ付けされていない PDF ファイルの表抽出をサポートする。

\subsection{PDF表情報抽出の手法}
編集不可能な文書を表現する最も広範な方法の1つとして、PDF ファイルは豊富な表構造と情報を持っており、PDF 表抽出のためのいくつかのツールと手段が登場している。
Ramel ら[2]は、表の表現スキームに基づいた手法を設計し、表の検出と抽出にグラフィックラインを使用した。彼らはまた、テキスト要素の規則性のみを利用することにより、グラフィックマークがほとんどない表を扱う手法も提案した。

TableSeer は、Liu ら[11]によって提案された表検索エンジンである。
これは、デジタルライブラリから科学文書 (主に PDF 文書) をクロールし、文書から表を識別し、表にインデックスを付けてランク付けし、ユーザーに検索インターフェースを提供する。
これは、表題、フォント、空白、その他の要素を使用して表検出に使用できるボックスカッティング法で構築されている。
このシステムは競争力があるものの、精度に大きく依存している。
すべての表に表題があると想定しているため、表が他のキーワードでラベル付けされている場合には失敗し、再現率が低くなる。
同様に、Perez-Arriaga ら[12]は、PDF 文書内の表から情報を自動的に検出、抽出し、整理することができるシステムTAO (TAble Organization) を提案した。
しかし、TAO は表検出と表認識の両方で TableSeer よりも優れているものの、このシステムはグローバルな特徴をコンパクトな表現に集約する傾向があり、オブジェクト検出の実行にはあまり適していない。

[13]および[14]におけるグラフィカルモデルもまた、性能向上を求めるために使用されてきた。
[14]では、Pintoらは、指定された入力ノード上の値の条件付き確率を計算するために使用できる無向グラフィカルモデルである条件付き確率場 (CRF) を表抽出に使用した。
彼らは、文書の各行にタグを付けて表の境界を示し、境界、行列表、ヘッダーセルを認識するようにモデルを訓練した。
テスト結果は良好だったが、モデルは列について何も知らず、データセルとヘッダーセルを区別することができない。

研究者らはまた、表抽出のために PDF を XML に変換することも試みた。
Yildiz ら[1]は pdf2table システムを提案し、その中で pdftohtml ツールによって返されたデータを表認識 (「構成要素」を表として識別する) に使用した。
次に、ヘッダー要素、スパニング動作 (すなわち、いくつの列または行がスパニングされているか) など、それらの中の情報を明らかにするために表を分解した。
ユーザーが調整を行い制限を克服するためのグラフィカルユーザーインターフェースを備えているものの、品質は pdftohtml ツールに大きく依存する。
特定の状況下 (例えば、抽出対象の表が画像である場合など) ではツールは有用な情報を返さず、これはユーザーによって適合させることができない。

\subsection{OpenCVに関する使用法}
OpenCV [15] は、オープンソースのコンピュータービジョンライブラリである。
これは多くの関数を提供し、多くのコンピュータービジョンアルゴリズムを効果的に実装する。

画像処理は常に OpenCV の最も重要な機能の1つであり、エッジ検出はその重要なセグメントである。
Xie と Lu [16] は、OpenCV に基づいて、細いワイヤ内の銅コアの正確な数を検出およびカウントする手法を実装した。
彼らは、形態学的オープニングおよびクロージング操作を使用して高解像度画像をセグメント化し、輪郭追跡によって数をカウントした。
Pásztó と Hubinský [17] は、移動ロボットナビゲーションに視覚システムを使用する可能性について議論した。
彼らは、ロボットで実行される標識検出の一部である円と線を検出するためにハフ変換を使用した。

OpenCV で広く使用されている色検出は、[18]、[19]、[20]の交通標識認識 (TSR) の分野、および、[21]、[22]、[23]の皮膚検出で利用されている。
[21]では、Oliveira と Conci が最初に RGB 画像を HSV 色空間に変換して、皮膚検出の色範囲を特徴付けている。

OpenCV を実行するために、我々は主に PDF 前処理のための色検出と、表認識および表セル値抽出のための水平線および垂直線検出に焦点を当てる。


\section{提案手法}
本手法のワークフローを図 1 に示す。
提案手法は、PDF 前処理、表検出、セル値抽出の 3 つの要素で構成する。

\begin{enumerate}
    \item \textbf{PDF前処理}:
    PDF 文書を画像に変換し、関心領域 (ROI) の輪郭を明確にするための前処理を行う。
    \item \textbf{表検出}:
    上記で得られた画像に基づき、OpenCV を用いて PDF ファイルの各ページ上の表を検出し、各画像内のすべての輪郭情報を取得する。
    \item \textbf{セル値抽出}:
    各表のセルを検出し、そこからテキストを抽出する。
    同時に、ページをまたがる表が存在するかどうかを判断し、存在する場合は前のページの表と結合する。
    判断と表形式の解釈を通じて、PDF 内の表からメタデータとデータを取得する。
\end{enumerate}

我々は OpenCV を利用して、多くの構築済みコンピュータービジョン機能にアクセスする。
また、Tesseract OCR エンジンも表セル値抽出のサポートのために使用する。
具体的に、それらがどのように機能するかを以下で説明する。

\subsection{PDF前処理}
まず、OpenCV は PDF 文書を直接処理できないため、PDF ファイルをページごとに画像に変換する。

特殊なケースとして、表の行や列に彩度の高い色が含まれている場合があり、表認識のための線検出が困難になることがある。
この問題を解決するために、以下の手順を使用する：

\begin{enumerate}
    \item \textbf{彩度の抽出}:
    「元画像」 (図 2(a)) の色空間を BGR (青、緑、赤) から HSV (色相、彩度、明度) に変換し、彩度を抽出する。
    結果を「彩度画像」 (図2(b)) として保存する。
    \item \textbf{二値画像の取得}:
    「彩度画像」を読み込み、グレースケール化、ガウシアンぼかし、その後、大津の二値化を画像に適用する。
    \item \textbf{関心領域 （ROI） の検出}:
    輪郭を見つけ、輪郭の周囲長と指定された精度の多角形曲線を用いた輪郭近似を使用してフィルタリングする。
    領域が長方形であるという仮定のもと、輪郭近似結果が 4 であれば目的の領域が見つかる。
    さらに、輪郭の面積を計算してノイズを除去する。
    \item \textbf{ROI を元画像に置換}:
    バウンディングボックスの座標を取得し、numpy スライシングで ROI を抽出することで、「元画像」に ROI を置換できるようにする。
    画像を「置換画像」 (図2(c)に示す) として保存する。
\end{enumerate}

% アルゴリズム 1 を入れる

アルゴリズム 1 について、P を PDF ファイル内のページ数、C を置換対象の輪郭の数とする。
PDF から画像への変換操作の計算量は O(1) である。
画像の背景を変更する操作は、ページ内で条件を満たすすべての輪郭を変更するため、その計算量は O(C) となる。
したがって、アルゴリズム 1 全体の計算量は O(PC) である。

\subsection{表検出}
次に、コンピュータービジョン技術に基づいて画像内の表を検出し、その位置を取得する。
画像内に複数の表がある場合は、表を特定し、行と列が交差する場所に基づいて切り出す。

\begin{enumerate}
    \item \textbf{画像の閾値処理}: 
    OpenCV では、輪郭を見つけることは、黒い背景から白いオブジェクトを見つけるようなものである。
    我々は表に重点を置いているため、閾値処理を使用して表の線を白に、その他の背景を黒に変換する。
    これを完了するために閾値処理を実行し、より高い精度を得るために二値画像も選択する。
    \item \textbf{水平線と垂直線の識別}:
    次に、形態学的操作を利用して表を検出する。
    最初に、画像の幅に基づいて長さを決定した長方形カーネルを定義することである。
    次に、画像内の水平線と垂直線を個別に検出する 2 つのカーネルを定義する。
    この手順では、識別の鍵となるのは形態学的操作である。
    これは形状に基づいた一連の画像処理操作であり、構造要素を入力画像に適用することによって出力画像を生成する。
    ここで、カーネルを使用して連続的に収縮と膨張を行う方法をとる。
    これらの線を識別しようとするにあたり、最も適切な水平カーネルと垂直カーネルを選択する方法をとる。
    これは、線を正確に識別し、画像内の表の特定に非常に役立つ。
    図 3 (a) と図 3 (b) に線の識別結果を示しており、それぞれ水平線と垂直線を示す。
    \item \textbf{すべての輪郭の取得}:
    画像内のすべての水平線と垂直線に関する 2 つの画像をそれぞれ取得した後、これら 2 つの画像の合計を取得する。
    これにより、元画像内の表の枠だけでなく、元の表内の情報も除外される。
    したがって、表を正確に特定し、誤った抽出に起因するノイズの問題を軽減できる。
    一連の操作の後、画像内の表の位置と形状を正常に取得する。
    結果を図 3(c) に示す。
    輪郭とは、同じ色または強度を持つ連続するすべての点（境界に沿った点）を結ぶ曲線として簡単に説明できる。
    輪郭は、形状分析、オブジェクト検出、および認識に役立つツールである[24]。
    findContours 関数を利用して、表自体と表内のすべてのセルを含む元画像内のすべての輪郭を取得し、セルの座標と幅、高さを取得する。
    後続の処理を容易にするために、画像内の各輪郭を上から下、左から右の方法で並べ替える。
\end{enumerate}










// ここまで翻訳済み

\section{文書画像解析}

文書画像解析 (DIA) は、画像処理モデルのあらゆるアプローチで必要に応じて使用される複数の段階で構成される。
以下のセクションでは、提案システムで使用され必要とされる主な画像解析の段階について説明する。


\subsection{前処理}

前処理は画像の二値化と画質向上を含み、特徴抽出のための重要なステップである。
そのため、線や表の検出ステップに進む前に、後続のステップのための結果の利便性を制御する。
したがって、前処理技術の使用は、抽出段階における次のステップのための準備として表検出を向上させることができる[10]。
この前処理ステップは、スキャンされた画像で一般的に見られるエッジの遷移のぼかしやその他のノイズの影響を軽減するために使用される。


\begin{figure}[tp]
    \begin{center}
        \includegraphics*[width=7cm]{image/master/Fig1.png}
        \caption{SWTの実装 (a)典型的ストローク (b)$u$はストロークの境界線上のピクセル。$v$は$u$における勾配方向のストローク境界の反対側のピクセル。
            (c)パスに沿った各ピクセルには、その現在値と求めたストローク幅の最小値を割り当てる(Epshteinら, 2010)。}
        \label{fig1}
    \end{center}
\end{figure}

\begin{figure}[tp]
    \begin{center}
        \includegraphics*[width=7cm]{image/master/Fig2.png}
        \caption{異なるストローク幅の文字がある文書画像}
        \label{fig2}
    \end{center}
\end{figure}



\begin{table*}[tp]
    \centering
    \caption{図\ref{fig2}のaにおけるストローク幅変換行列の値}
    \label{table1}
    \begin{tabular}{cccccccccccc}
        \hline
        15.07 & 15.07 & 15.07 & 15.07 & 15.07 & 15.46 & 15.46 & 15.46 & 15.46 & 15.46 & 15.46 & 15.46 \\
        15.07 & 15.07 & 15.07 & 15.43 & 15.43 & 15.43 & 15.43 & 15.43 & 15.43 & 15.43 & 15.43 & 15.43 \\
        12.81 & 14.87 & 14.87 & 14.87 & 15.36 & 15.36 & 15.36 & 15.36 & 15.36 & 15.36 & 15.36 & 15.36 \\
        13.75 & 14.97 & 14.97 & 14.97 & 14.97 & 14.97 & 14.97 & 15.36 & 15.36 & 15.36 & 15.36 & 15.36 \\
        13.75 & 14.42 & 14.97 & 14.97 & 14.97 & 14.97 & 14.97 & 14.97 & 14.97 & 14.97 & 15.36 & 15.36 \\
        13.75 & 14.42 & 14.42 & 15.00 & 15.00 & 15.00 & 15.00 & 15.00 & 15.00 & 15.00 & 15.00 & 15.00 \\
        13.75 & 14.42 & 14.42 & 15.00 & 15.00 & 15.00 & 15.00 & 15.00 & 15.00 & 15.00 & 15.00 & 15.00 \\
        14.39 & 14.39 & 14.39 & 15.00 & 15.00 & 15.00 & 15.00 & 15.00 & 15.00 & 15.00 & 15.00 & 15.00 \\
        14.39 & 14.39 & 14.39 & 14.87 & 14.87 & 14.87 & 14.87 & 14.87 & 14.87 & 14.87 & 14.87 & 14.87 \\
        14.28 & 14.28 & 14.97 & 14.97 & 14.97 & 14.97 & 14.97 & 14.97 & 14.97 & 14.97 & 14.97 & 14.97 \\
        15.07 & 15.07 & 15.07 & 15.07 & 15.07 & 15.07 & 15.07 & 15.07 & 15.07 & 15.07 & 15.07 & 15.07 \\
        15.07 & 15.07 & 15.07 & 15.07 & 15.07 & 15.07 & 15.07 & 15.07 & 15.07 & 15.07 & 15.07 & 15.07 \\
        15.07 & 15.07 & 15.07 & 15.07 & 15.07 & 15.07 & 15.07 & 15.07 & 15.07 & 15.07 & 15.33 & 15.33 \\
        15.00 & 15.00 & 15.00 & 15.00 & 15.00 & 15.00 & 15.00 & 15.33 & 15.33 & 15.33 & 15.33 & 15.33 \\
        15.00 & 15.00 & 15.00 & 15.00 & 15.00 & 15.33 & 15.33 & 15.33 & 15.33 & 15.33 & 15.33 & 15.33 \\
        14.14 & 14.87 & 14.87 & 15.36 & 15.36 & 15.36 & 15.36 & 15.36 & 15.36 & 15.36 & 15.36 & 15.36 \\
        14.14 & 15.36 & 15.36 & 15.36 & 15.36 & 15.36 & 15.36 & 15.36 & 15.36 & 15.36 & 15.36 & 15.36 \\
        10.00 & 15.36 & 15.36 & 15.36 & 15.36 & 15.36 & 15.36 & 15.36 & 15.36 & 15.36 & 15.36 & 15.46 \\
        9.75  & 15.33 & 15.33 & 15.33 & 15.33 & 15.33 & 15.33 & 15.33 & 15.33 & 15.33 & 15.33 & 15.49 \\
        \hline
    \end{tabular}
\end{table*}


\begin{table*}[tp]
    \centering
    \caption{図\ref{fig2}のbにおけるストローク幅変換行列の値}
    \label{table2}
    \begin{tabular}{cccccccccc}
        \hline
        10.77 & 9.75  & 9.75  & 9.75  & 10.00 & 10.00 & 10.82 & 10.15 & 10.15 & 10.15 \\
        10.77 & 9.75  & 9.75  & 9.75  & 9.75  & 9.75  & 9.59  & 9.75  & 9.75  & 9.95  \\
        10.77 & 9.75  & 9.90  & 9.90  & 9.90  & 9.75  & 9.75  & 9.75  & 9.75  & 9.75  \\ 
        10.77 & 9.75  & 9.90  & 9.90  & 9.90  & 9.90  & 9.75  & 9.75  & 9.75  & 9.75  \\
        10.77 & 9.75  & 9.90  & 9.90  & 9.90  & 9.75  & 9.75  & 9.75  & 9.75  & 10.00 \\
        10.77 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 9.75  & 10.00 & 10.00 & 10.00 \\
        10.77 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 \\
        10.77 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 \\
        10.77 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 \\
        10.77 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 \\
        10.34 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 \\
        10.34 & 9.75  & 10.00 & 10.00 & 10.00 & 9.75  & 9.59  & 10.34 & 10.82 & 10.82 \\
        10.34 & 9.59  & 9.75  & 10.00 & 9.75  & 9.59  & 9.54  & 14.87 & 14.87 & 14.87 \\
        \hline
    \end{tabular}
\end{table*}


\subsection{Sauvola法の改良}

Sauvola法(Sauvola and Pietaksinen, 2000)では、パラメータであるウィンドウサイズ(W)と$k$は固定であり、特定の文書画像に対してこれら2つの変数の値を正しく設定することが不可欠である。
しかし、それぞれの文書画像に対して、手作業でこれらのパラメータの正確な値を計算し、設定することは困難である。
また、文書画像には、様々なサイズやストローク幅のテキストが含まれている可能性がある。
単一のウィンドウサイズは、あるサイズのテキストに対してはうまく機能するが、他のサイズに対してはうまく機能しない。
Modified Sauvola'sと名付けた提案手法は、ストローク幅変換を用いて計算されたストローク幅に基づいて、各ピクセルの近傍サイズを自動的に計算する。
アルゴリズムは以下の通りである:

\textbf{ステップ1:} \par
\noindent 入力画像がカラー画像であれば、グレースケール画像に変換する。

\textbf{ステップ2:} \par
\noindent (\ref{sec2.2}節で説明したように)入力画像のストローク幅変換(SWT)を計算し、SWT行列を生成する。

\textbf{ステップ3:} \par
\noindent SWT行列を使用して、各画素のウィンドウサイズを以下のように自動的に計算する：

\begin{equation}\label{eq2}
    W(i, j) = 4 \times SW(i, j) + 1
\end{equation}

実験的には、式\ref{eq1}で与えられるウィンドウサイズが最良の結果となる。
位置$(i, j)$の画素のウインドウサイズ$W(i, j) \times W(i, j)$を閾値の計算に用い、同画像内の各ピクセルで異なる。

\textbf{ステップ4:} \par
\noindent 位置$(i, j)$の画素の閾値を、ステップ3と式\ref{eq2}からそのピクセルのウィンドウサイズを用いて推定する（Sauvola法と同じ）。



\section{実験結果}

実験は、DIBCOベンチマークデータセットDIBCO-2009(Anon, 0000a)\cite{bib26}、HDIBCO-2010(Anon, 0000b)\cite{bib27}、
DIBCO-2011(Anon, 0000c)\cite{bib28}、HDIBCO-2012(Anon, 0000d)\cite{bib29}、HDIBCO-2016(Anon, 0000e)\cite{bib30}、
およびDIBCO-2017(Anon, 0000f)\cite{bib31}から選択した劣化文書画像に対して行う。
これらのデータセットには、様々な実際の劣化文書画像と、それに対応する半自動生成したグラウンド・トゥルースを含む。
本節では、定量的な実験結果とOCRに基づく実験結果について述べる。
提案手法を既存の9つの適応的二値化手法と比較する:
Otsu (1979)、Bernsen (1986)、Niblack (1986)、Wellner (1993)、
Sauvola と Pietaksinen (2000)、Wolf と Jolion (2003)、Singhら (2012)。
Otsu法は大域的二値化手法であり、他の手法は適応的二値化手法である。
これらの適応的二値化手法はすべて、Sauvola法と同様に2つのパラメータを手動で調整するものであり、いずれの二値化結果もこれらのパラメータの値に大きく影響を受ける。

\subsection{統計的結果}

統計的結果は、定量的尺度を用いて評価する：
% ピーク信号対雑音比: 画質の再現性に影響を与える、信号が取りうる最大のパワーと劣化をもたらすノイズの比率を表す工学用語
% 画像がどれだけ劣化をしたかを示す値。値が小さいほど劣化していて、大きいほど元の画像に近い。
F値(FM)、ピーク信号対雑音比(PSNR)、
% 画像処理タスクにおける背景の誤検出率を評価する指標。通常は、背景と誤って認識された前景領域の割合を示す。
負率メトリック(Negative Rate Metric, NRM)、
誤分類ペナルティメトリック(Misclassification penalty metric, MPM)、
距離相互歪みメトリック(Distance Reciprocal Distortion Metric, DRD)。
% 文書解析と認識に関する国際会議 ICDAR
% ICDAR2015は、風景写真中のテキスト領域にアノテーションされた1000枚のトレーニング画像と500枚のテスト画像
これらの評価指標は、国際的文書画像の二値化結果の比較(Gatosら, 2009; Pratikakisら, 2016, 2017)
で提案されたICDARベンチマーク評価指標から採用した。
これらの評価指標は、取得した二値画像とグラウンド・トゥルースの画像との類似度を定義する。

\textbf{FM}は適合率(PR)と再現率(RC)を組み合わせ、総合的に二値化精度を決定する。
適合率(PR)は二値画像でのノイズを、再現率(RC)はテキスト本文の精度をそれぞれ示す。
これら3つの値が高いほど、出力する二値画像($I_B$)が理想的な二値画像($I_{GT}$)に近いことを示す。

\begin{equation}\label{eq3}
    FM = \frac{2 \times RC \times PR}{RC + PR}
\end{equation}

$PR = \frac{N_{TT}}{N_{FT} + N_{TT}}$、$RC = \frac{N_{TT}}{N_{FNT} + N_{TT}}$とする。

ここで、$N_{TT}$は真のテキストピクセルの数、$N_{FT}$は偽のテキストピクセルの数、$N_{FNT}$は偽の非テキストピクセルの数である。

\textbf{PSNR}は出力結果が理想的な二値画像に近いかどうかを測定する指標である。
PSNRの値が高いほど、二値化の品質が良いことを示す。

\begin{equation}\label{eq4}
    PSNR = 10 \times \log (\frac{D^2}{MSE}),
\end{equation}

$D$は画像のコントラストである。二値画像の場合、$D$の値は1とする。$MSE$は、平均二乗誤差である。

\begin{equation}\label{eq5}
    MSE = \sum_{i=1}^M \sum_{j=1}^N \frac{ (I_B(i, j) - I_{GT}(i, j))^2 }{MN}
\end{equation}

$I_B(i, j)$は理想的な二値画像の画素値、$I_{GT}(i, j)$は同ピクセルにおける理想的な画素値をそれぞれ示す。

\textbf{NRM}は、$I_{GT}$と$I_B$の間のピクセル非類似率を測定する。
$NRM$の値が低いほど、二値化が適切であることを示す。

\begin{equation}\label{eq6}
    NRM = \frac{P + Q}{2}
\end{equation}

ここで、
\begin{equation}\label{eq7}
    P = \frac{N_{FNT}}{N_{FNT} + N_{TT}},
\end{equation}

\begin{equation}\label{eq8}
    Q = \frac{N_{FT}}{N_{FT} + N_{TNT}}
\end{equation}
である。

\textbf{MPM}は、以下のように定義されている。

\begin{equation}\label{eq9}
    MPM = \frac{ \sum_{i=1}^{C_{FNT}} d_{FNT}^i + \sum_{j=1}^{C_{FT}} d_{FT}^j }{2D}
\end{equation}

ここで、$d_{FNT}^i$は偽の非テキストの距離を表し、
$d_{FT}^j$は$j$番目の偽のテキストピクセルについて、グラウンド・トゥルースにおけるテキスト輪郭からの距離を表す。
正規化係数$D$は、グラウンド・トゥルースのすべてのピクセルからテキスト輪郭までの距離の総和である。
この指標は、出力した二値画像がどれだけのグラウンド・トゥルースの輪郭を表現しているかを表す。
$MPM$の値が小さいほど、二値化アルゴリズムの質が高いことを示す。

\textbf{DRD}は二値化した文書画像の視覚的な歪みを測定する指標であり、Luら(2004)によって導入された。
この指標は、画像に対する人間の視覚的知覚と手法の性能の間に相関があることを示す。

上記のすべてのデータセットを用いた提案手法と、Sauvola法の平均定量結果を、表\ref{table3}に示す。
表\ref{table3}の定量的結果と図\ref{fig3}、図\ref{fig4}、図\ref{fig5}の視覚的結果から、すべての種類の劣化画像(機械印刷と手書き)に対して、
提案手法がSauvola法よりも良い結果を示すことがわかる。


\begin{table*}[tp]
    \centering
    \caption{異なるデータセットを用いたSauvolaと提案手法の定量的比較}
    \label{table3}
    \begin{tabular}{cccccccccc}
        \toprule
                           &  & \multicolumn{2}{c}{DIBCO-2009 PR} & \multicolumn{2}{c}{DIBCO-2009HW} & \multicolumn{2}{c}{DIBCO-2011PR}                                 \\
                           &  & Sauvola                           & Proposed                         & Sauvola                          & Proposed & Sauvola & Proposed \\
        \midrule
        FM\%               &  & 68.69                             & 85.39                            & 51.13                            & 64.68    & 67.76   & 77.39    \\
        Recall \%          &  & 53.77                             & 75.47                            & 40.84                            & 54.94    & 52.19   & 64.74    \\
        Prec.\%            &  & 99.32                             & 98.84                            & 97.41                            & 95.21    & 99.70   & 98.96    \\
        PSNR               &  & 11.89                             & 14.51                            & 15.48                            & 16.34    & 12.48   & 13.85    \\
        NRM                &  & 23.15                             & 12.34                            & 29.60                            & 22.60    & 23.91   & 17.66    \\
        MPM ($\times1000$) &  & 2.57                              & 1.05                             & 0.45                             & 0.38     & 0.88    & 0.63     \\
        DRD                &  & 12.06                             & 7.18                             & 10.95                            & 8.63     & 9.82    & 6.57     \\
        \midrule
                           &  & \multicolumn{2}{c}{DIBCO-2011 HW} & \multicolumn{2}{c}{HDIBCO-2016}  & \multicolumn{2}{c}{DIBCO-2017}                                   \\
                           &  & Sauvola                           & Proposed                         & Sauvola                          & Proposed & Sauvola & Proposed \\
        \midrule
        FM\%               &  & 62.62                             & 67.75                            & 73.74                            & 85.74    & 39.25   & 52.31    \\
        Recall \%          &  & 49.00                             & 58.24                            & 61.26                            & 85.445   & 30.69   & 43.88    \\
        Prec.\%            &  & 98.00                             & 95.01                            & 98.77                            & 87.44    & 83.63   & 95.21    \\
        PSNR               &  & 14.70                             & 15.59                            & 16.33                            & 17.34    & 12.27   & 12.81    \\
        NRM                &  & 25.00                             & 21.06                            & 19.41                            & 7.81     & 34.70   & 27.14    \\
        MPM ($\times1000$) &  & 1.50                              & 1.48                             & 1.26                             & 1.01     & 5.00    & 1.40     \\
        DRD                &  & 9.14                              & 8.40                             & 8.99                             & 8.28     & 14.13   & 10.44    \\
        \bottomrule
    \end{tabular}
\end{table*}

\begin{figure}[tp]
    \begin{center}
        \includegraphics*[width=7cm]{image/master/Fig3.png}
        \caption{(a) 入力画像 (DIBCO-2009のP02.bmp)、(b) Sauvola法の出力(F値 = 77.14)、(c) 提案手法 (F値 = 91.72)}
        \label{fig3}
    \end{center}
\end{figure}

\begin{figure}[tp]
    \begin{center}
        \includegraphics*[width=7cm]{image/master/Fig4.png}
        \caption{(a) 入力画像 (DIBCO-2009のH04.bmp)、(b) Sauvola法の出力 (F値 = 73.15)、(c) 提案手法 (F値 = 85.79)}
        \label{fig4}
    \end{center}
\end{figure}

\begin{figure}[tp]
    \begin{center}
        \includegraphics*[width=7cm]{image/master/Fig5.png}
        \caption{(a) DIBCO-2011データセットの画像PR4.pngの一部(テキストが密集し、ストロークが変化)、(b)グラウンド・トゥルース、(c)Otsu法、(d)Niblack法、(e)Bernsen法、(f)Wellner法、(g)Sauvola法、(h)Wolf法、(i)Bradley と Rothの方法、(j)NICK法、(k)Singhら(2012)の方法、(l)提案手法}
        \label{fig5}
    \end{center}
\end{figure}

また、ストロークの幅と文字サイズが変化する印刷文書画像に対しても実験を行った。
DIBCO-2011の劣化文書画像PR1.png、PR3.png、PR4、PR6、DIBCO-2009のP02.bmp、P03.bmp、
DIBCO-2017データセットの13.bmp、14.bmp、15.bmp、20.bmpの10枚を選択し、
選択した既存技術と提案手法の平均統計指標を、表\ref{table4}に示す。
結果、提案手法はこのような種類の画像に対して、他の手法よりも優れていることがわかった。


\begin{table*}[tp]
    \centering
    \caption{ストローク幅が異なり密集したテキストがある画像に対する様々な手法の定量的比較}
    \label{table4}
    \begin{tabular}{lllllll}
        \hline
                                       & 再現率(\%) & 適合率(\%) & FM(\%)         & PSNR           & NRM            & MPM($\times$1000) \\
        \hline
        Otsu (1979)                    & 95.23      & 68.54      & 74.14          & 12.79          & 13.41          & 104.55            \\
        Bernsen (1986)                 & 89.99      & 28.69      & 41.99          & 5.14           & 22.10          & 213.38            \\ 
        Niblack (1986)                 & 85.35      & 30.75      & 43.53          & 5.69           & 21.82          & 185.99            \\
        Wellner (1993)                 & 45.52      & 91.83      & 59.04          & 11.52          & 27.49          & 4.91              \\
        Sauvola and Pietaksinen (2000) & 59.27      & 98.41      & 73.11          & 13.17          & 20.43          & 2.24              \\
        Wolf and Jolion (2003)         & 70.01      & 95.21      & 79.56          & 14.79          & 15.22          & 2.45              \\
        Bardley and Roth (2007)        & 79.68      & 80.17      & 78.50          & 13.32          & 12.36          & 12.47             \\
        Khurshidら (2009)              & 70.57      & 87.86      & 76.51          & 13.21          & 15.28          & 6.65              \\
        Singhら (2012)                 & 58.78      & 81.68      & 65.12          & 11.57          & 21.59          & 11.42             \\
        提案手法                       & 75.51      & 96.68      & \textbf{84.81} & \textbf{15.01} & \textbf{12.30} & \textbf{1.63}     \\
        \hline
    \end{tabular}
\end{table*}


ストローク幅とテキストサイズが変化する劣化文書画像について、異なる手法の視覚的結果(図\ref{fig5})でも、同様であるとわかる。
図\ref{fig5}の結果は、Niblack (1986)とBernsen (1986)の二値化手法では、非テキスト領域に黒いノイズが発生することを示している。
Wellner (1993)、 Sauvola と Pietaksinen (2000), Wolf と Jolion (2003), Bardley と Roth (2007), 
Khurshidら (2009)、Singhら (2012) の各二値化手法は、文字が密集した画像や文字のサイズが変わる画像からテキストピクセルを完全に取り出すことができない。
提案手法は、このような画像に対して最良の結果となる。
低コントラスト画像に対する実験では、Sauvola法と同様に、提案手法はそのような画像に対して良好な二値化結果が得られない。
図\ref{fig6}のF値は、Sauvola法と比較して、提案手法がこのような種類の画像に対してもより多くの真の画素を取得することを示している。

\begin{figure}[t]
    \begin{center}
        \includegraphics*[width=7cm]{image/master/Fig6.png}
        \caption{(a) 入力画像(HDIBCO-2010の低コントラスト画像H06.tif)、(b) Sauvola法(F値 = 31.13)、(c) 提案手法(F値 = 39.68)}
        \label{fig6}
    \end{center}
\end{figure}


\subsection{OCR基準の評価}

二値化の性能は、光学式文字認識(OCR)システムの認識過程に直接影響する。
「レーベンシュタイン距離とは、2つの文字列の類似度を表す指標である。
その距離は、ある文字列を別の文字列に変換するのに必要な削除、挿入、置換の数である」と、Levenshteinは定義した。
手書きOCRでは満足な結果が得られないため、この方法は機械印刷文書の評価にのみ使用できる。
提案手法と既存手法の二値化結果を、無料のオンラインOCR (Anon, 0000g)\cite{bib32}を使って分析する。
より高品質な二値化を行うためには、グラウンド・トゥルースのOCR結果と、二値画像のOCR結果とのレーベンシュタイン距離を、より小さくする必要がある。
データセットから選択した画像の一部について、提案手法および他の手法でのOCR結果によるレーベンシュタイン距離を、図\ref{fig7}に示す。
この結果は、ストロークの幅とテキストのサイズが変化する画像に対して、提案手法が最良の性能であることを示している。


\begin{figure*}[t]
    \begin{center}
        \includegraphics*[width=7cm]{image/master/Fig7.png}
        \caption{DIBCO-2011データセットの画像 PR3.png の一部に対するOCR結果とレーベンシュタイン距離}
        \label{fig7}
    \end{center}
\end{figure*}


\section{結論}

本論文では、最新の二値化手法であるSauvolaを改良した適応的二値化手法を提案した。
Sauvola法では、ウィンドウサイズのパラメータは手動で設定する必要があり、
異なる領域におけるテキストの特性の変化にかかわらず、画像全体に対して固定である。
提案する手法は、ストローク幅変換行列を用いてウィンドウサイズを動的に計算する。
視覚的および定量的な結果を示し、印刷画像および手書き画像において、
提案手法の性能がSauvola法と比較して向上していることを示す。
ストローク幅や文字サイズが変化する画像に対して、提案手法は、Otsu、Niblack、Bernsen、Wellner、Sauvola、Wolf、Bradley と Roth、NICK、Singhの二値化手法の二値化結果を上回る。


\section{訳者の感想}
劣化ではないが、撮影環境の影響によって画像品質が悪い帳票に対して、より適切に二値化できる可能性があると考え、この論文を選択した。
実際にあったのは、両面印刷の紙を撮影したときに、裏に書いている文字が透けてしまい、文字認識がうまくいかなかったことがあった。
この論文を見る限り、そのような場合にも対応できそうであると考えた。
また、ウインドウサイズの調節によって、一部に色がついた帳票に対しても適切に二値化できる可能性があると考えた。

\begin{thebibliography}{99}
    \bibitem{bib1}
    Otsu, N.,
    \newblock “A threshold selection method from gray level histograms”,
    \newblock { {\em IEEE Trans. Syst. Man Cybern}, Vol. 9, Issue 1, 1979, pp. 62–66. }
    
    \bibitem{bib2}
    Pun, T.,
    \newblock “A new method for gray-level picture threshold using the entropy of the histogram”,
    \newblock { {\em Signal Processing 2}, Vol. 2, Issue 3, 1980, pp. 223-237. }
    
    \bibitem{bib3}
    Pun, T.,
    \newblock {“Entropic thresholding: a new approach”},
    \newblock { {\em Computer Graphics and Image Processing}, Vol. 16, Issue 3, 1981, pp. 210-239.}
    
    \bibitem{bib4}
    Johannsen, G. and Bille, J.,
    \newblock “A threshold selection method using information measures”,
    \newblock { {\em Proc. International Conference on Pattern Recognition}, 1982, pp. 140-143. }
    
    \bibitem{bib5}
    Kapur, J.N., Sahoo, P.K., A.K.C., Wong.,
    \newblock “A new method for gray-level picture thresholding using the entropy of the histogram”,
    \newblock { {\em Computer Graphics and Image Processing}, Vol. 29, Issue 3, 1985, pp. 273-285. }
    
    \bibitem{bib6}
    Kittler, J., Illingworth, J.,
    \newblock “On Threshold Selection Using Clustering Criteria”,
    \newblock { {\em IEEE Transactions on Systems, Man and Cybernetics}, Volume SMC-15, Issue 5, 1985, pp. 652-655. }
    
    \bibitem{bib7}
    Abutaleb, A.S.,
    \newblock “Automatic thresholding of gray-level pictures using two- dimensional entropy”,
    \newblock { {\em Computer Graphics and Image Processing}, Vol. 47, Issue 1, 1989, pp. 22-32. }
    
    \bibitem{bib8}
    Brink, A.D., Pendock, N.E.,
    \newblock “Minimum cross entropy threshold selection. Pattern Recognit”,
    \newblock { {\em Computer Graphics and Image Processing}, Vol. 29, Issue 1, 1996, pp. 179-188. }
    
    \bibitem{bib9}
    Bernsen, J.,
    \newblock “Dynamic thresholding of gray level images”,
    \newblock { {\em Proceedings - International Conference on Pattern Recognition}, Vol. 3, No. 1, 1986, pp. 1251–1255. }
    
    \bibitem{bib10}
    Niblack, W.,
    \newblock { {\em An Introduction to Image Processing. Prentice-Hall, Englewood Cliffs}, pp. 115-116. }
    
    \bibitem{bib11}
    Wellner, P.,
    \newblock { {\em Adaptive Thresholding for the Digital Desk. Xerox, EPC1993-110}, 1993. }
    
    \bibitem{bib12}
    Sauvola, J., Pietaksinen, M.,
    \newblock “Adaptive document image binarization”,
    \newblock { {\em Pattern Recognition}, Vol. 33, Issue 2, 2000, pp. 225-236. }
    
    \bibitem{bib13}
    Yang, Y., Yan, H., 
    \newblock “An adaptive logical method for binarization of degraded document images”,
    \newblock { {\em Pattern Recognition}, Vol. 33, Issue 5, 2000, pp. 787–807. }
    
    \bibitem{bib14}
    Wolf, C., Jolion, J.M.,
    \newblock “Extraction and recognition of artificial text in multimedia documents”
    \newblock { {\em Journal of Family Violence}, Vol. 18, Issue 6, 2003, pp. 309–316. }
    
    \bibitem{bib15}
    Do, J., He, Q.D.M., Downton, A.C., Kim, J.H.,
    \newblock “A comparison of binarization methods for historical archive documents”
    \newblock { {\em In: Eighth International Conference on Document Analysis and Recognition (ICDAR’05)}, Vol. 1, Seoul, South Korea, 2005, pp. 538–542. }
    
    \bibitem{bib16}
    Gatos, B., Ntirogiannis, K., Pratikakis, I.,
    \newblock “ICDAR 2009 document image binarization contest (DIBCO 2009)”,
    \newblock { {\em In: 10th International Conference on Document Analysis and Recognition}, Barcelona, 2009, pp. 1375–1382 }
    
    \bibitem{bib17}
    Bardley, D., Roth, G.,
    \newblock “Adaptive Thresholding using the Integral Image”.
    \newblock { {\em Journal of Graphics GPU and Game Tools}, Vol. 12, Issue 2, 2007, pp. 13-21. }
    
    \bibitem{bib18}
    Shafait, F., Keysers, D., Bruel, T.M.,
    \newblock “Efficient implementation of local adaptive thresholding techniques using integral images”,
    \newblock { {\em In: Proceedings of SPIE 6815 on Document Recognition and Retrieval XV}, 2018}
    
    \bibitem{bib19}
    Khurshid, K., Siddiqi, I., Faure, C., Vincent, N.,
    \newblock “Comparison of Niblack inspired methods for ancient document”,
    \newblock { {\em In: Proceedings 16th IEEE International Conference on Document Recognition and Retrieval}, Vol. 7247, 2009, pp. 1–10. }
    
    \bibitem{bib20}
    Zhou, S., Liu, C., Cui, Z., Gong, S., 
    \newblock “An improved adaptive document image binarization method”,
    \newblock { {\em In: Proceedings of 2nd IEEE International Conference on Image Signal Processing}, Vol. 7, 2009, pp. 1–5. }
    
    \bibitem{bib21}
    Kawano, H., Oohama, H., Maeda, H., Okada, Y., Ikoma, N.,
    \newblock “Degraded document image binarization combining local statistics”,
    \newblock { {\em In: IEEE International Joint Conference (ICROS-SICE).}, 2009, pp. 439–443. }
    
    \bibitem{bib22}
    Lu, S., Tan, C.L.
    \newblock “Document image binarization using background estimation and stroke edges”,
    \newblock { {\em In: IEEE International Joint Conference (ICROS-SICE).}, Vol. 13, 2010, pp. 303–314. }
    
    \bibitem{bib23}
    Singh, T.R., Roy, S., Singh, O.I., Sinam, T., Singh, K.M.,
    \newblock “A new local adaptive thresholding technique in binarization”,
    \newblock { {\em Communications in Computer and Information Science}, Vol. 8, Issue 6, 2011, pp. 271-277. }
    
    \bibitem{bib24}
    Singh, O.I., Sinam, T., James, O., Singh, T.R.,
    \newblock “Local contrast and mean based thresholding technique in binarization”,
    \newblock { {\em International Journal of Computer Applications}, Vol. 51, No. 6, 2012, pp. 5-10. }
    
    
    \bibitem{bib25}
    Natarajan, J., Sreedevi, I.,
    \newblock “Enhancement of ancient manuscript images by log based binarization technique”,
    \newblock { {\em AEU - International Journal of Electronics and Communications}, Vol. 75, No. 6, 2017, pp. 15-22. }
    
    \bibitem{bib26}
    \newblock "Anon",
    \newblock {users.iit.demokritos.gr/~bgat/DIBCO2009/benchmark}
    
    \bibitem{bib27}
    \newblock "Anon",
    \newblock {https://users.iit.demokritos.gr/~bgat/H-DIBCO2010/benchmark/}
    
    \bibitem{bib28}
    \newblock "Anon",
    \newblock {utopia.duth.gr/~ipratika/DIBCO2011/resources}
    
    \bibitem{bib29}
    \newblock "Anon",
    \newblock {utopia.duth.gr/ipratika/HDIBCO2012/benchmark}
    
    \bibitem{bib30}
    \newblock "Anon",
    \newblock {https://vc.ee.duth.gr/h-dibco2016/benchmark.}
    
    \bibitem{bib31}
    \newblock "Anon",
    \newblock {https://vc.ee.duth.gr/dibco2017}
    
    \bibitem{bib32}
    \newblock "Anon",
    \newblock "IMAGE TO TEXT CONVERTER - OCR ONLINE",
    \newblock {https://www.onlineocr.net/}
\end{thebibliography}

\end{document}
